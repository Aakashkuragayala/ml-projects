# -*- coding: utf-8 -*-
"""deeplearning_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mE_x9LvlRkYgcruNPPeDXt80fCnitJlk
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

# Load the CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Preprocess the data
x_train = x_train / 255.0
x_test = x_test / 255.0
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the ConvNext architecture
def convnext(input_shape, num_classes):
    inputs = Input(shape=input_shape)
    x = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)

    # Residual blocks
    for i in range(3):
        shortcut = x
        x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)
        x = BatchNormalization()(x)
        x = Add()([shortcut, x])
        x = ReLU()(x)

    # Dense block
    for i in range(3):
        shortcut = x
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=128, kernel_size=(1, 1), padding='same')(x)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)
        x = tf.concat([x, shortcut], axis=-1)

    # Output layer
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    # Define the model
    model = Model(inputs=inputs, outputs=outputs)
    return model

# Define the model
model = convnext((32, 32, 3), 10)

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=5, batch_size=12, validation_data=(x_test, y_test))

# Plot the training and validation loss and accuracy
fig, axs = plt.subplots(2, figsize=(10,10))
axs[0].plot(history.history['loss'], label='Training Loss')
axs[0].plot(history.history['val_loss'], label='Validation Loss')
axs[0].legend()
axs[1].plot(history.history['accuracy'], label='Training Accuracy')
axs[1].plot(history.history['val_accuracy'], label='Validation Accuracy')
axs[1].legend()
plt.show()

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test)
print("Test Loss: {:.4f}, Test Accuracy: {:.4f}".format(test_loss, test_acc))